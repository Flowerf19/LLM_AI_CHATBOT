"""
BatchProcessor - X·ª≠ l√Ω batch tin nh·∫Øn v√† g·ª≠i cho AI t√≥m t·∫Øt
V2.1: T√≠ch h·ª£p Context Overlap ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh li√™n t·ª•c gi·ªØa c√°c batch
"""
import json
from datetime import datetime
from typing import List
from pathlib import Path

from src.utils.helpers import get_logger
from src.data.data_manager import data_manager
from src.services.conversation.recent_log_service import recent_log_service
from src.services.conversation.pending_update_service import pending_update_service
from src.services.ai.ollama_service import OllamaService 
from src.services.ai.gemini_service import GeminiService
from src.config.settings import Config

from src.models.v2.recent_log import Activity
from src.models.v2.batch_summary import BatchSummary, CriticalEvent
from src.models.v2.user_summary import UserSummary, CriticalEventHistory

logger = get_logger(__name__)

# Use absolute path relative to project root
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
USER_PROFILE_DIR = str(PROJECT_ROOT / "data" / "user_profiles")
PROMPT_PATH = Config.PROMPTS_DIR / "batch_summary_prompt.json"


class BatchProcessor:
    """
    X·ª≠ l√Ω batch tin nh·∫Øn v·ªõi Context Overlap (V2.1):
    1. L·∫•y batch + context t·ª´ RecentLog
    2. Format data cho AI v·ªõi context overlap
    3. G·ªçi AI ƒë·ªÉ t√≥m t·∫Øt v√† ph√°t hi·ªán critical events
    4. X·ª≠ l√Ω critical events v√† Lazy Sync
    """
    
    def __init__(self):
        # Ch·ªçn AI service d·ª±a v√†o config
        ai_provider = getattr(Config, 'ai_provider', 'ollama').lower()
        self.ai_service = GeminiService() if ai_provider == 'gemini' else OllamaService()
        self._prompt_template = None

    async def _load_prompt(self):
        if not self._prompt_template:
            try:
                async with data_manager._get_lock(str(PROMPT_PATH)):
                    with open(PROMPT_PATH, "r", encoding="utf-8") as f:
                        self._prompt_template = json.load(f)
            except Exception as e:
                logger.error(f"Failed to load prompt template: {e}")
                # Fallback prompt c·ª±c k·ª≥ c∆° b·∫£n n·∫øu file l·ªói
                self._prompt_template = {
                    "system": "You are a JSON summarizer.",
                    "format": "{}"
                }

    async def process_batch(self, server_id: str = "default"):
        """
        Main Flow V2.1:
        1. L·∫•y tin nh·∫Øn t·ª´ RecentLog (batch + context overlap)
        2. G·ª≠i cho AI ph√¢n t√≠ch v·ªõi context
        3. N·∫øu c√≥ Critical Event -> Update v√†o UserSummary + Lazy Sync
        4. Reset Batch Tracker
        """
        # 1. L·∫•y d·ªØ li·ªáu v·ªõi context overlap
        active_batch, context_msgs = await recent_log_service.get_batch_for_processing()
        
        if not active_batch:
            logger.debug("No batch to process")
            return

        logger.info(f"üîÑ Processing batch: {len(active_batch)} messages + {len(context_msgs)} context")
        
        # 2. Chu·∫©n b·ªã Prompt v·ªõi Context Overlap
        await self._load_prompt()
        full_prompt = self._build_prompt_with_context(context_msgs, active_batch)

        # 3. G·ªçi AI
        try:
            raw_response = await self.ai_service.generate(full_prompt)
            
            # Clean response (g·ª° markdown ```json ... ``` n·∫øu c√≥)
            clean_json = self._extract_json(raw_response)
            parsed_data = json.loads(clean_json)
            
            # Debug: Log AI response
            logger.debug(f"ü§ñ AI Response: {parsed_data.get('summary', 'N/A')}")
            logger.debug(f"üîç Detected Events: {len(parsed_data.get('critical_events', []))}")
            
            # Validate b·∫±ng Pydantic
            batch_summary = BatchSummary(
                batch_id=f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                timestamp=datetime.now(),
                messages_count=len(active_batch),
                user_ids=list(set([msg.user_id for msg in active_batch])),
                ai_summary=parsed_data.get("summary", ""),
                has_critical_events=parsed_data.get("has_critical_events", False),
                critical_events=[
                    CriticalEvent(**evt) for evt in parsed_data.get("critical_events", [])
                ],
                processing_time=0.0,
                llm_model=self.ai_service.__class__.__name__
            )
            
            logger.info(f"‚úÖ AI Analysis complete. Critical Events: {len(batch_summary.critical_events)}")
            if batch_summary.critical_events:
                for evt in batch_summary.critical_events:
                    logger.info(f"  üìå {evt.event_type}: {evt.summary} (confidence: {evt.confidence})")

            # 4. X·ª≠ l√Ω Critical Events v·ªõi Lazy Sync (V2.1)
            if batch_summary.has_critical_events:
                await self._handle_critical_events(
                    batch_summary.critical_events, 
                    batch_summary.batch_id,
                    server_id
                )

        except Exception as e:
            logger.error(f"‚ùå Batch AI Error: {e}", exc_info=True)
        
        # 5. Reset Tracker (quan tr·ªçng ƒë·ªÉ kh√¥ng b·ªã k·∫πt)
        await recent_log_service.reset_batch_tracker()
    
    def _build_prompt_with_context(
        self, 
        context_messages: List[Activity], 
        current_batch: List[Activity]
    ) -> str:
        """
        X√¢y d·ª±ng prompt cho AI v·ªõi Context Overlap (V2.1).
        Format: Context (read-only) + Current Batch (analyze)
        """
        # Format context messages
        context_text = ""
        if context_messages:
            context_text = "--- CONTEXT (READ ONLY) ---\n"
            context_text += "These are previous messages for context:\n"
            for msg in context_messages:
                time_str = msg.timestamp.strftime("%H:%M")
                context_text += f"[{time_str}] {msg.username}: {msg.content}\n"
            context_text += "\n"
        
        # Format current batch
        batch_text = "--- MESSAGES TO ANALYZE ---\n"
        for msg in current_batch:
            time_str = msg.timestamp.strftime("%H:%M")
            batch_text += f"[{time_str}] {msg.username} ({msg.user_id}): {msg.content}\n"
        
        # Build full prompt
        full_prompt = (
            f"{self._prompt_template.get('instruction', 'Analyze these messages')}\n\n"
            f"{context_text}"
            f"{batch_text}\n"
            f"--- OUTPUT FORMAT ---\n{json.dumps(self._prompt_template.get('format_example', {}), indent=2)}"
        )
        
        return full_prompt
    
    def _extract_json(self, text: str) -> str:
        """Extract JSON t·ª´ text, x·ª≠ l√Ω markdown code blocks n·∫øu c√≥"""
        text = text.strip()
        
        if "```json" in text:
            start = text.find("```json") + 7
            end = text.find("```", start)
            return text[start:end].strip()
        elif "```" in text:
            start = text.find("```") + 3
            end = text.find("```", start)
            return text[start:end].strip()
        
        return text

    async def _handle_critical_events(
        self, 
        events: List[CriticalEvent], 
        batch_id: str,
        server_id: str = "default"
    ):
        """
        Update UserSummary d·ª±a tr√™n s·ª± ki·ªán quan tr·ªçng.
        V2.1: T√≠ch h·ª£p Lazy Sync - t·∫°o pending updates cho affected users
        """
        for event in events:
            user_id = event.user_id
            if not user_id: 
                continue

            # Load User Profile
            profile_path = f"{USER_PROFILE_DIR}/{user_id}/summary.json"
            
            def default_user():
                return UserSummary(user_id=user_id, username=event.username)

            user_summary = await data_manager.load_model(
                profile_path, 
                UserSummary, 
                default_factory=default_user
            )

            # Convert to History Event
            history_event = CriticalEventHistory(
                event_id=f"evt_{int(datetime.now().timestamp())}",
                timestamp=event.timestamp,
                event_type=event.event_type,
                summary=event.summary,
                confidence=event.confidence,
                batch_id=batch_id,
                detected_at=datetime.now(),
                status="active",
                affected_relationships=event.affected_users
            )
            
            # Update & Save User A (ng∆∞·ªùi t·∫°o event)
            user_summary.critical_events.append(history_event)
            user_summary.last_updated = datetime.now()
            user_summary.total_events_tracked += 1
            
            await data_manager.save_model(profile_path, user_summary)
            logger.info(f"üíæ Updated UserSummary for {event.username}: {event.event_type}")
            
            # V2.1: Lazy Sync - T·∫°o pending updates cho affected users
            if event.affected_users:
                for affected_user_id in event.affected_users:
                    if affected_user_id != user_id:  # Kh√¥ng t·∫°o pending cho ch√≠nh m√¨nh
                        await pending_update_service.add_pending_update(
                            target_user_id=affected_user_id,
                            source_event_id=history_event.event_id,
                            update_type="relationship_sync",
                            data={
                                "from_user_id": user_id,
                                "from_username": event.username,
                                "event_type": event.event_type,
                                "summary": event.summary,
                                "timestamp": event.timestamp.isoformat()
                            },
                            server_id=server_id
                        )
                        logger.info(f"üìå Created pending update for user {affected_user_id}")

# Global instance
batch_processor = BatchProcessor()